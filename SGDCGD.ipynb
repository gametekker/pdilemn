{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Player 1 Beliefs: {'CC': 0.21, 'CB': 0.68, 'BC': 0.83, 'BB': 0.67}\n",
      "Player 2 Beliefs: {'CC': 0.09, 'CB': 0.19, 'BC': 0.12, 'BB': 0.5}\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from modules.utility import generate_player_beliefs\n",
    "\n",
    "# Example usage\n",
    "P1_beliefs, P2_beliefs = generate_player_beliefs()\n",
    "\n",
    "# Define the rewards and payoffs\n",
    "r1, r2 = 3, 3  # Reward for mutual cooperation\n",
    "t1, t2 = 5, 5  # Temptation payoff\n",
    "p1, p2 = 0, 0  # Punishment payoff\n",
    "s1, s2 = 1, 1  # Sucker's payoff\n",
    "gamma = 0.9,.6,.3,.2,.1    # Discount rate\n",
    "\n",
    "# Define the rewards matrix\n",
    "rewards_matrix = np.array([\n",
    "    [r1, r2],  # Payoffs for when both cooperate\n",
    "    [t1, p2],  # Payoffs for when one defects and the other cooperates\n",
    "    [p1, t2],  # Payoffs for when one defects and the other cooperates\n",
    "    [s1, s2]   # Payoffs for when both defect\n",
    "])\n",
    "\n",
    "print(\"Player 1 Beliefs:\", P1_beliefs)\n",
    "print(\"Player 2 Beliefs:\", P2_beliefs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3, 5, 0, 1]\n",
      "[3, 0, 5, 1]\n",
      "tensor([[3.6560, 3.5983],\n",
      "        [5.6826, 0.5472],\n",
      "        [0.6826, 5.5472],\n",
      "        [1.6560, 1.5983]], grad_fn=<MmBackward0>)\n",
      "done testing\n",
      "tensor([[3.6820, 3.6378],\n",
      "        [5.7049, 0.5950],\n",
      "        [0.7049, 5.5950],\n",
      "        [1.6820, 1.6378]], grad_fn=<MmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "from modules.Player import Player\n",
    "from modules.PlayerEnv import PlayerEnv\n",
    "from torch import optim\n",
    "\n",
    "pl1,pl2=Player(P1_beliefs,[r1,t1,p1,s1]),Player(P2_beliefs,[r2,p2,t2,s2])\n",
    "\n",
    "# Define optimizers for each player\n",
    "# Assuming that Player class has a method 'parameters()' that returns its parameters\n",
    "optimizer_p1 = optim.SGD(pl1.parameters(), lr=0.01)\n",
    "optimizer_p2 = optim.SGD(pl2.parameters(), lr=0.01)\n",
    "\n",
    "env=PlayerEnv(pl1,pl2,.2)\n",
    "\n",
    "print(env.play())\n",
    "\n",
    "print(\"done testing\")\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 100\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "    reward = env.play()\n",
    "    optimizer_p1.zero_grad()\n",
    "    rp1 = reward[:, 0]\n",
    "    loss_p1 = -rp1.sum()\n",
    "    loss_p1.backward()\n",
    "    optimizer_p1.step()\n",
    "    \n",
    "    reward = env.play()\n",
    "    optimizer_p2.zero_grad()\n",
    "    rp2 = reward[:, 1]\n",
    "    loss_p2 = -rp2.sum()\n",
    "    loss_p2.backward()\n",
    "    optimizer_p2.step()\n",
    "\n",
    "    # Logging\n",
    "    #if epoch % 10 == 0:\n",
    "    #    print(f\"Epoch {epoch}: pl1 reward = {rp1.sum().item()}, pl2 reward = {rp2.sum().item()}\")\n",
    "\n",
    "print(env.play())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3, 5, 0, 1]\n",
      "[3, 0, 5, 1]\n",
      "tensor([[3.6531, 3.6011],\n",
      "        [5.6797, 0.5501],\n",
      "        [0.6797, 5.5501],\n",
      "        [1.6531, 1.6011]], grad_fn=<MmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "from CGDs import ACGD\n",
    "lr = 0.0001\n",
    "\n",
    "pl1=Player(P1_beliefs,[r1,t1,p1,s1])\n",
    "pl2=Player(P2_beliefs,[r2,p2,t2,s2])\n",
    "env=PlayerEnv(pl1,pl2,.2)\n",
    "optimizer = ACGD(max_params=pl1.parameters(), min_params=pl2.parameters(), lr_max=lr, lr_min=lr)\n",
    "\n",
    "# max_parems is maximizing the objective function while the min_params is trying to minimizing it. \n",
    "# BCGD(max_params=G.parameters(), min_params=D.parameters(), lr_max=lr, lr_min=lr, device=device)\n",
    "# ACGD: Adaptive CGD;\n",
    "for epoch in range(100):\n",
    "    reward=-env.play().sum()\n",
    "    optimizer.zero_grad()\n",
    "    optimizer.step(loss=reward)\n",
    "\n",
    "print(env.play())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
